{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHqRQDH50Bv2kj8GXM8KKW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashanksrajak/master-generative-ai/blob/main/gen-ai-course-google/1_prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "A prompt is just an input given to the LLMs in plain English to get some response back. How we structure our prompt is crucial in getting the expected and reliable response from LLMs. Hence, prompt engineering is an active area of development and is of utmost importance in LLM world and AI Engineering.\n",
        "\n",
        "Follow this codelab by Google (source)\n",
        "https://www.kaggle.com/code/markishere/day-1-prompting"
      ],
      "metadata": {
        "id": "BdbruNhYX4tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages\n",
        "\n",
        "`pip install google-genai`"
      ],
      "metadata": {
        "id": "eYRyCW5DZ8tJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CwdMkNhlXt9M"
      },
      "outputs": [],
      "source": [
        "# genai package from google to use gemini models\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# most of the output generated by LLM is in markdown\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup API key to use the model\n",
        "\n",
        "We can also setup this in `env` variables and load the `env`"
      ],
      "metadata": {
        "id": "bP9FQYfga6J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "MG1x2iEcawcm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a model client\n",
        "Which will be used to invoke the model and get the response"
      ],
      "metadata": {
        "id": "Sh3Ddp13bB1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    contents = \"Hi, what's your vibe?\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBYCSGTabHFZ",
        "outputId": "3fca9fe9-418b-4fd7-e2f9-727c1de1113f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a large language model, I don't have personal feelings or a \"vibe\" in the human sense. However, I can try to describe the impression I aim to give:\n",
            "\n",
            "*   **Helpful and Informative:** My primary goal is to assist you with your requests by providing accurate and comprehensive information.\n",
            "*   **Objective and Neutral:** I strive to present information without bias and avoid expressing personal opinions.\n",
            "*   **Clear and Concise:** I aim to communicate in a way that is easy to understand and avoids unnecessary jargon.\n",
            "*   **Empathetic and Respectful:** I recognize that you have a specific need, and I try to address it with sensitivity and consideration.\n",
            "*   **Creative and Imaginative (when appropriate):** If you ask me to write a story or poem, I can tap into my creative abilities.\n",
            "\n",
            "So, if I were to try and capture it in a single \"vibe\" description, it would be something like: **\"Intelligent, supportive, and reliable—like a highly knowledgeable and patient assistant.\"**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "pwFsHh23cOOZ",
        "outputId": "48854f08-0072-4cc6-ad0e-0139a76479d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As a large language model, I don't have personal feelings or a \"vibe\" in the human sense. However, I can try to describe the impression I aim to give:\n\n*   **Helpful and Informative:** My primary goal is to assist you with your requests by providing accurate and comprehensive information.\n*   **Objective and Neutral:** I strive to present information without bias and avoid expressing personal opinions.\n*   **Clear and Concise:** I aim to communicate in a way that is easy to understand and avoids unnecessary jargon.\n*   **Empathetic and Respectful:** I recognize that you have a specific need, and I try to address it with sensitivity and consideration.\n*   **Creative and Imaginative (when appropriate):** If you ask me to write a story or poem, I can tap into my creative abilities.\n\nSo, if I were to try and capture it in a single \"vibe\" description, it would be something like: **\"Intelligent, supportive, and reliable—like a highly knowledgeable and patient assistant.\"**\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start a Chat\n",
        "Instead of single turn message, we can start a chat."
      ],
      "metadata": {
        "id": "WSTv8QAccWO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    history = []\n",
        ")\n",
        "\n",
        "response = chat.send_message(\"Hi, my name is Shashank\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGmu8SPpcQMr",
        "outputId": "827baa44-c17a-4dfc-c174-c096d01e66f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Shashank, it's nice to meet you! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# send another message to this chat\n",
        "\n",
        "response = chat.send_message('Can you tell me something interesting about Samosa?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM3pHkpCc5YQ",
        "outputId": "aa554b47-0faf-4e13-c260-a0b2db30d30b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, Shashank, here's an interesting fact about samosas:\n",
            "\n",
            "While samosas are deeply associated with Indian cuisine, **they are believed to have originated in the Middle East!**\n",
            "\n",
            "Specifically, the earliest mentions of a dish resembling the samosa can be traced back to the 10th or 11th century in the Middle East, under the name \"Sambusak\" or \"Sanbusak.\" These were small, triangular pastries filled with minced meat, nuts, and dried fruits.\n",
            "\n",
            "From the Middle East, the dish traveled along trade routes to India, where it was adapted with local ingredients and spices, eventually becoming the samosa we know and love today.\n",
            "\n",
            "So, next time you're enjoying a samosa, you can think about its surprising journey from the Middle East to your plate!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See, how it used my name in this response as if it already knew my name and that is because of we are using `chats` instead of single-turn respone."
      ],
      "metadata": {
        "id": "t-edoZGHdG24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Do you remember what my name is?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMRzMBoWdQKT",
        "outputId": "681273a1-f64e-44dc-fe0d-1ecd9413a5b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Shashank! I remember your name is Shashank.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List all the models available with Google\n"
      ],
      "metadata": {
        "id": "KhjrlqC1dbbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "low8D2txdd2B",
        "outputId": "e5146115-e1b4-4e8a-e182-d30745406bfb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/veo-2.0-generate-001\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.0-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMzurHY9dlkN",
        "outputId": "0cca9f11-fdba-4728-a771-0dc469b43c92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent',\n",
            "                       'countTokens',\n",
            "                       'createCachedContent',\n",
            "                       'batchGenerateContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Parameters"
      ],
      "metadata": {
        "id": "5ygDb3A-d1ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### output token limit"
      ],
      "metadata": {
        "id": "BmF08vi4d45v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "short_config = types.GenerateContentConfig(maxOutputTokens=100)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=short_config,\n",
        "    contents='Write a 1000 word essay on LLM models.')\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEBkCe65d4BY",
        "outputId": "7740c42c-b15e-4e34-e9c7-f59798f5f318"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Rise of the Machines: Exploring the Capabilities and Challenges of Large Language Models\n",
            "\n",
            "Large Language Models (LLMs) have rapidly emerged as a transformative technology, capturing the imagination of researchers, developers, and the public alike. These sophisticated artificial intelligence systems, trained on massive datasets of text and code, possess an uncanny ability to generate human-quality text, translate languages, summarize information, and even answer complex questions. While the potential applications of LLMs are vast and varied, ranging from revolutionizing customer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "tptDJ5PHeouZ",
        "outputId": "42c95343-3f0c-4e58-b636-a9b1a5e7f4cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## The Rise of the Machines: Exploring the Capabilities and Challenges of Large Language Models\n\nLarge Language Models (LLMs) have rapidly emerged as a transformative technology, capturing the imagination of researchers, developers, and the public alike. These sophisticated artificial intelligence systems, trained on massive datasets of text and code, possess an uncanny ability to generate human-quality text, translate languages, summarize information, and even answer complex questions. While the potential applications of LLMs are vast and varied, ranging from revolutionizing customer"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output token limit will just stop the model generating new tokens once it has hit the limit provided by us. See the last word output, its incomplete!!!"
      ],
      "metadata": {
        "id": "xbx8wF9IevF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature\n",
        "\n",
        "Higher temp means more creative response."
      ],
      "metadata": {
        "id": "bvpvUJTXfKtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will output different colors each time (randomness)"
      ],
      "metadata": {
        "id": "jOJGtwk1fVO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=high_temp_config,\n",
        "      contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "  if response.text:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MVC8-hge3eA",
        "outputId": "497808f6-2759-4da0-ce38-0bcfa1bdab0e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orange\n",
            " -------------------------\n",
            "Teal\n",
            " -------------------------\n",
            "Turquoise\n",
            " -------------------------\n",
            "Turquoise\n",
            " -------------------------\n",
            "Cerulean.\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will output same colors everytime."
      ],
      "metadata": {
        "id": "DfIKBkOKfcBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=0)\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=high_temp_config,\n",
        "      contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "  if response.text:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlSeMeNlfZX5",
        "outputId": "a3202424-b56a-4bd5-b632-e29923904534"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-p\n",
        "\n",
        "Selects a token randomly from all the tokens whose cumulative probability adds up to p."
      ],
      "metadata": {
        "id": "BON4OaVEfycf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a dog who goes on a space voyage.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=story_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plkja7suf4oV",
        "outputId": "fd9c066e-99ec-4bee-e17f-3eeb779f4373"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosmo wasn't your typical Golden Retriever. Sure, he loved belly rubs and chasing squirrels with the ferocious dedication only a Golden can muster. But Cosmo also dreamed of stars. Every night, he'd sit on the porch, head tilted, a single floppy ear twitching, and howl at the constellations. He yearned for the cosmic kibble, the planetary playtime, the celestial smells he imagined were drifting out there.\n",
            "\n",
            "His human, a quirky astrophysicist named Dr. Aris Thorne, noticed. He saw the longing in Cosmo's soulful brown eyes, the silent plea to explore the unknown. So, Dr. Thorne, in his infinite eccentricity, did something utterly bonkers. He snuck Cosmo aboard his new experimental spacecraft, the \"Star Hopper,\" just hours before launch.\n",
            "\n",
            "The Star Hopper wasn't built for comfort, let alone canines. Cosmo, stuffed into a custom-made doggie spacesuit, felt like a furry sausage in a shiny silver casing. But the roar of the engines, the vibrating floor, and the g-force pushing him into his padded seat were all secondary to the thrill that coursed through him. He was leaving Earth!\n",
            "\n",
            "Space was…unexpected. The blackness was deeper, the stars brighter, and the smells… indescribable. There was the metallic tang of the ship, the ozone-rich scent of the ion drive, and a strange, almost nutty aroma that Cosmo suspected was space dust.\n",
            "\n",
            "Dr. Thorne, juggling navigation consoles and a squeaky toy, managed to keep Cosmo entertained. He tossed space-dog biscuits (specially formulated, of course) into the zero-gravity cabin, much to Cosmo's delight. They even played fetch with a modified tennis ball that had its own tiny thrusters.\n",
            "\n",
            "Their mission was to scout a potentially habitable planet orbiting a distant binary star system. After a few weeks of hyperspace jumps and nebula navigations, they arrived. The planet, dubbed \"Woof-topia\" by Dr. Thorne, was breathtaking. Lush green forests, sparkling blue oceans, and…giant, glowing, edible-looking plants.\n",
            "\n",
            "Dr. Thorne, bundled in his own suit, opened the airlock. Cosmo, leash extended, bounded onto the surface, his tail wagging so hard it nearly spun him around. The air was sweet, the gravity lighter, and the giant glowing plants tasted remarkably like bacon.\n",
            "\n",
            "Woof-topia was a canine paradise. Cosmo chased alien butterflies with iridescent wings, rolled in fields of soft, purple grass, and even befriended a small, furry creature that looked like a cross between a squirrel and a teddy bear.\n",
            "\n",
            "But the best part? He finally understood the music of the stars. It wasn't a howl, but a symphony of silent vibrations, a language spoken in the fabric of the universe. He was home.\n",
            "\n",
            "Of course, their mission was temporary. They had to return to Earth, to report their findings. But Cosmo knew, deep in his furry heart, that this was just the beginning. He was Cosmo, the Space Dog, and his adventures were only just starting. He looked up at the binary stars, shining like two cosmic dog treats, and barked a joyous, extraterrestrial howl. The universe, he knew, was his for the fetching.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting"
      ],
      "metadata": {
        "id": "U-OkmGsLgr5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### zero shot prompt"
      ],
      "metadata": {
        "id": "sKhpKu7agthH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_output_tokens=5,\n",
        ")\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Lagaan\" is fascinating movie that shows the India during British era and the dismal state of the society. It shows how Cricket played a crucial role in uplifting the state of the village people.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrImVwsKgs_X",
        "outputId": "3746fc8e-9dec-40f6-d4bc-29535d61d83c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ),\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPVkVp2OhNjP",
        "outputId": "e81ff87b-751d-47fb-b327-ac914984cd32"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few shot prompt\n",
        "Give few examples to make the model learn"
      ],
      "metadata": {
        "id": "i8VqqaeJhgH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large pizza with cheese & pineapple\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ),\n",
        "    contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxhklYDfhkPk",
        "outputId": "f9de2df1-c69f-400d-eb04-38822645829e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above response is good but we will need extra efforts to clean the extra characters and create a valid `json` hence we will use JSON mode."
      ],
      "metadata": {
        "id": "t1CyzhEeiK5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder, # pass the response schema for output\n",
        "    ),\n",
        "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHSpETTeiTl9",
        "outputId": "ccc7330f-c907-4b2b-ab1e-35cc15824eac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we didn't provide any `few-shot` examples, and just by the response schema, we got desired output that too in json format that can be used in downstream applications."
      ],
      "metadata": {
        "id": "zNgQV3csidxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of thought"
      ],
      "metadata": {
        "id": "3D-Fl0lVi2qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvzzcGMwinRn",
        "outputId": "fe6af2f1-128c-4b33-93a6-a17de483d07e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think step by step. - this prompt made the difference!"
      ],
      "metadata": {
        "id": "6gBvirTHi_dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "ojusbPZKi61m",
        "outputId": "5e78e116-a361-40e5-eabb-dd412902f7ca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve this:\n\n1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n3. **Determine the partner's current age:** Since the age difference remains constant, your partner is always 8 years older than you. Now that you're 20, your partner is 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**\n"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct : Reason and Act"
      ],
      "metadata": {
        "id": "pnICeKTpjU09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mEhz8oDWjXbl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "# You will perform the Action; so generate up to, but not including, the Observation.\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences=[\"\\nObservation\"],\n",
        "    system_instruction=model_instructions + example1 + example2,\n",
        ")\n",
        "\n",
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "react_chat = client.chats.create(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=react_config,\n",
        ")\n",
        "\n",
        "resp = react_chat.send_message(question)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6e4er7qkNVj",
        "outputId": "d6d019bf-7f0d-4492-96a7-ba5b61b1bc1d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the Transformers NLP paper and then find the authors listed. I then need to determine who the youngest author is.\n",
            "\n",
            "Action 1\n",
            "<search>Transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n",
        "resp = react_chat.send_message(observation)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-fQV1wukuHX",
        "outputId": "a883e4cb-795c-44e3-856c-6c2c0728f7f9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 2\n",
            "The paper is \"Attention Is All You Need\" by Vaswani et al. I need to search each author to find the youngest.\n",
            "\n",
            "Action 2\n",
            "<search>Ashish Vaswani</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thinking Mode"
      ],
      "metadata": {
        "id": "jE9Kvcbrlb7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        ")\n",
        "\n",
        "buf = io.StringIO()\n",
        "for chunk in response:\n",
        "    buf.write(chunk.text)\n",
        "    # Display the response as it is streamed\n",
        "    print(chunk.text, end='')\n",
        "\n",
        "# And then render the finished response as formatted markdown.\n",
        "clear_output()\n",
        "Markdown(buf.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "tWSsucnlldLs",
        "outputId": "d2500f8b-6d06-4a11-bc0c-193c60f0671a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Determining the *exact* youngest author based on their birth dates is difficult as that specific personal information is not typically public.\n\nHowever, based on their known professional status and backgrounds at the time the \"Attention Is All You Need\" paper was published in 2017, the author widely believed to be the youngest was **Aidan N. Gomez**.\n\nAt the time of the paper's publication, Aidan N. Gomez was an undergraduate student intern at Google Brain, where he contributed to the research. The other authors were generally more senior researchers and engineers with established careers or advanced degrees, making him the likely candidate for the youngest."
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ),\n",
        "    contents=code_prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "LJXLAbeblxna",
        "outputId": "971ad7b3-5e2b-4afc-983f-7c35d3ac2a42"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  \"\"\"\n  Calculates the factorial of a non-negative integer.\n\n  Args:\n    n: The integer for which to calculate the factorial.\n\n  Returns:\n    The factorial of n. Returns 1 if n is 0, and raises ValueError if n is negative.\n  \"\"\"\n  if n < 0:\n    raise ValueError(\"Factorial is not defined for negative numbers.\")\n  elif n == 0:\n    return 1\n  else:\n    result = 1\n    for i in range(1, n + 1):\n      result *= i\n    return result\n```"
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}